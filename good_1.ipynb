{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing shakespeare text\n",
    "f = open('data/shakespeare.txt', 'r')\n",
    "\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line)\n",
    "    \n",
    "lst_text = [line for line in lines if len(line.split()) > 1]\n",
    "\n",
    "text = ''\n",
    "for char in lst_text:\n",
    "    text+= char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94285"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "step_size = 5 \n",
    "seq_len = 40 \n",
    "for i in range(0, len(text) - seq_len, step_size):\n",
    "    X.append('' + text[i:seq_len + i])\n",
    "    y.append('' + text[i + seq_len])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18849"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = sorted(list(set(text)))\n",
    "char_to_num, num_to_char = {}, {}\n",
    "count = 0\n",
    "for char in alphabet:\n",
    "    char_to_num[char] = count\n",
    "    num_to_char[count] = char\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_code = [char_to_num[label] for label in y]\n",
    "x_code= []\n",
    "for seq in X:\n",
    "    x_code.append([char_to_num[char] for char in seq])\n",
    "    \n",
    "final_X = np_utils.to_categorical(x_code)\n",
    "final_y = np_utils.to_categorical(y_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18849, 40, 61)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape= (final_X.shape[1], final_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "#CHECK THE ACTIVATION\n",
    "model.add(Dense(final_y.shape[1], activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 3.0567 - acc: 0.1823\n",
      "Epoch 00001: loss improved from inf to 3.05585, saving model to weights-improvement-01-3.0559.hdf5\n",
      "18849/18849 [==============================] - 53s 3ms/step - loss: 3.0559 - acc: 0.1824\n",
      "Epoch 2/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.5268 - acc: 0.2937\n",
      "Epoch 00002: loss improved from 3.05585 to 2.52680, saving model to weights-improvement-02-2.5268.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 2.5268 - acc: 0.2939\n",
      "Epoch 3/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.3162 - acc: 0.3351\n",
      "Epoch 00003: loss improved from 2.52680 to 2.31594, saving model to weights-improvement-03-2.3159.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 2.3159 - acc: 0.3348\n",
      "Epoch 4/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.2124 - acc: 0.3538\n",
      "Epoch 00004: loss improved from 2.31594 to 2.21208, saving model to weights-improvement-04-2.2121.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 2.2121 - acc: 0.3538\n",
      "Epoch 5/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.1326 - acc: 0.3766\n",
      "Epoch 00005: loss improved from 2.21208 to 2.13167, saving model to weights-improvement-05-2.1317.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 2.1317 - acc: 0.3769\n",
      "Epoch 6/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.0678 - acc: 0.3940\n",
      "Epoch 00006: loss improved from 2.13167 to 2.06717, saving model to weights-improvement-06-2.0672.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 2.0672 - acc: 0.3941\n",
      "Epoch 7/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 2.0135 - acc: 0.4087\n",
      "Epoch 00007: loss improved from 2.06717 to 2.01382, saving model to weights-improvement-07-2.0138.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 2.0138 - acc: 0.4086\n",
      "Epoch 8/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.9509 - acc: 0.4240\n",
      "Epoch 00008: loss improved from 2.01382 to 1.95066, saving model to weights-improvement-08-1.9507.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.9507 - acc: 0.4242\n",
      "Epoch 9/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.8987 - acc: 0.4374\n",
      "Epoch 00009: loss improved from 1.95066 to 1.89823, saving model to weights-improvement-09-1.8982.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.8982 - acc: 0.4376\n",
      "Epoch 10/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.8506 - acc: 0.4528\n",
      "Epoch 00010: loss improved from 1.89823 to 1.85044, saving model to weights-improvement-10-1.8504.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.8504 - acc: 0.4528\n",
      "Epoch 11/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.8111 - acc: 0.4595\n",
      "Epoch 00011: loss improved from 1.85044 to 1.81073, saving model to weights-improvement-11-1.8107.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.8107 - acc: 0.4598\n",
      "Epoch 12/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.7737 - acc: 0.4675\n",
      "Epoch 00012: loss improved from 1.81073 to 1.77343, saving model to weights-improvement-12-1.7734.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.7734 - acc: 0.4676\n",
      "Epoch 13/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.7261 - acc: 0.4793\n",
      "Epoch 00013: loss improved from 1.77343 to 1.72666, saving model to weights-improvement-13-1.7267.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.7267 - acc: 0.4790\n",
      "Epoch 14/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.6883 - acc: 0.4855\n",
      "Epoch 00014: loss improved from 1.72666 to 1.68819, saving model to weights-improvement-14-1.6882.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.6882 - acc: 0.4856\n",
      "Epoch 15/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.6467 - acc: 0.5014\n",
      "Epoch 00015: loss improved from 1.68819 to 1.64640, saving model to weights-improvement-15-1.6464.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.6464 - acc: 0.5016\n",
      "Epoch 16/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.6011 - acc: 0.5126\n",
      "Epoch 00016: loss improved from 1.64640 to 1.60215, saving model to weights-improvement-16-1.6022.hdf5\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 1.6022 - acc: 0.5121\n",
      "Epoch 17/150\n",
      "18400/18849 [============================>.] - ETA: 11s - loss: 1.5549 - acc: 0.5206\n",
      "Epoch 00017: loss improved from 1.60215 to 1.55727, saving model to weights-improvement-17-1.5573.hdf5\n",
      "18849/18849 [==============================] - 461s 24ms/step - loss: 1.5573 - acc: 0.5205\n",
      "Epoch 18/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.5059 - acc: 0.5355\n",
      "Epoch 00018: loss improved from 1.55727 to 1.50544, saving model to weights-improvement-18-1.5054.hdf5\n",
      "18849/18849 [==============================] - 54s 3ms/step - loss: 1.5054 - acc: 0.5357\n",
      "Epoch 19/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.4578 - acc: 0.5491\n",
      "Epoch 00019: loss improved from 1.50544 to 1.45837, saving model to weights-improvement-19-1.4584.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 1.4584 - acc: 0.5490\n",
      "Epoch 20/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.3943 - acc: 0.5685\n",
      "Epoch 00020: loss improved from 1.45837 to 1.39451, saving model to weights-improvement-20-1.3945.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 1.3945 - acc: 0.5685\n",
      "Epoch 21/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.3392 - acc: 0.5849\n",
      "Epoch 00021: loss improved from 1.39451 to 1.33930, saving model to weights-improvement-21-1.3393.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.3393 - acc: 0.5848\n",
      "Epoch 22/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.2836 - acc: 0.6021\n",
      "Epoch 00022: loss improved from 1.33930 to 1.28391, saving model to weights-improvement-22-1.2839.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.2839 - acc: 0.6017\n",
      "Epoch 23/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.6190\n",
      "Epoch 00023: loss improved from 1.28391 to 1.23179, saving model to weights-improvement-23-1.2318.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.2318 - acc: 0.6191\n",
      "Epoch 24/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.1726 - acc: 0.6341\n",
      "Epoch 00024: loss improved from 1.23179 to 1.17259, saving model to weights-improvement-24-1.1726.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.1726 - acc: 0.6342\n",
      "Epoch 25/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.1093 - acc: 0.6559\n",
      "Epoch 00025: loss improved from 1.17259 to 1.10997, saving model to weights-improvement-25-1.1100.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.1100 - acc: 0.6557\n",
      "Epoch 26/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.0419 - acc: 0.6787\n",
      "Epoch 00026: loss improved from 1.10997 to 1.04214, saving model to weights-improvement-26-1.0421.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 1.0421 - acc: 0.6786\n",
      "Epoch 27/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.6934\n",
      "Epoch 00027: loss improved from 1.04214 to 0.98706, saving model to weights-improvement-27-0.9871.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.9871 - acc: 0.6931\n",
      "Epoch 28/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.9285 - acc: 0.7123\n",
      "Epoch 00028: loss improved from 0.98706 to 0.92780, saving model to weights-improvement-28-0.9278.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.9278 - acc: 0.7128\n",
      "Epoch 29/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7291\n",
      "Epoch 00029: loss improved from 0.92780 to 0.87846, saving model to weights-improvement-29-0.8785.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.8785 - acc: 0.7292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7483\n",
      "Epoch 00030: loss improved from 0.87846 to 0.82333, saving model to weights-improvement-30-0.8233.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.8233 - acc: 0.7480\n",
      "Epoch 31/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7606\n",
      "Epoch 00031: loss improved from 0.82333 to 0.77820, saving model to weights-improvement-31-0.7782.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.7782 - acc: 0.7603\n",
      "Epoch 32/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.7383 - acc: 0.7702\n",
      "Epoch 00032: loss improved from 0.77820 to 0.73869, saving model to weights-improvement-32-0.7387.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.7387 - acc: 0.7702\n",
      "Epoch 33/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.7859\n",
      "Epoch 00033: loss improved from 0.73869 to 0.69025, saving model to weights-improvement-33-0.6902.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.6902 - acc: 0.7856\n",
      "Epoch 34/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7952\n",
      "Epoch 00034: loss improved from 0.69025 to 0.66193, saving model to weights-improvement-34-0.6619.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.6619 - acc: 0.7952\n",
      "Epoch 35/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.8111\n",
      "Epoch 00035: loss improved from 0.66193 to 0.62283, saving model to weights-improvement-35-0.6228.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.6228 - acc: 0.8110\n",
      "Epoch 36/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8244\n",
      "Epoch 00036: loss improved from 0.62283 to 0.58495, saving model to weights-improvement-36-0.5849.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.5849 - acc: 0.8241\n",
      "Epoch 37/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8294\n",
      "Epoch 00037: loss improved from 0.58495 to 0.55977, saving model to weights-improvement-37-0.5598.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.5598 - acc: 0.8294\n",
      "Epoch 38/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.8371\n",
      "Epoch 00038: loss improved from 0.55977 to 0.53485, saving model to weights-improvement-38-0.5349.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.5349 - acc: 0.8369\n",
      "Epoch 39/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8420\n",
      "Epoch 00039: loss improved from 0.53485 to 0.51526, saving model to weights-improvement-39-0.5153.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.5153 - acc: 0.8420\n",
      "Epoch 40/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8546\n",
      "Epoch 00040: loss improved from 0.51526 to 0.48235, saving model to weights-improvement-40-0.4823.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.4823 - acc: 0.8546\n",
      "Epoch 41/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.4690 - acc: 0.8572\n",
      "Epoch 00041: loss improved from 0.48235 to 0.46893, saving model to weights-improvement-41-0.4689.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.4689 - acc: 0.8571\n",
      "Epoch 42/150\n",
      "18800/18849 [============================>.] - ETA: 1s - loss: 0.4411 - acc: 0.8676\n",
      "Epoch 00042: loss improved from 0.46893 to 0.44121, saving model to weights-improvement-42-0.4412.hdf5\n",
      "18849/18849 [==============================] - 479s 25ms/step - loss: 0.4412 - acc: 0.8676\n",
      "Epoch 43/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8695\n",
      "Epoch 00043: loss improved from 0.44121 to 0.42589, saving model to weights-improvement-43-0.4259.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.4259 - acc: 0.8696\n",
      "Epoch 44/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8756\n",
      "Epoch 00044: loss improved from 0.42589 to 0.41116, saving model to weights-improvement-44-0.4112.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.4112 - acc: 0.8756\n",
      "Epoch 45/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8842\n",
      "Epoch 00045: loss improved from 0.41116 to 0.38214, saving model to weights-improvement-45-0.3821.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.3821 - acc: 0.8842\n",
      "Epoch 46/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8886\n",
      "Epoch 00046: loss improved from 0.38214 to 0.36737, saving model to weights-improvement-46-0.3674.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.3674 - acc: 0.8886\n",
      "Epoch 47/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8912\n",
      "Epoch 00047: loss improved from 0.36737 to 0.35641, saving model to weights-improvement-47-0.3564.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3564 - acc: 0.8912\n",
      "Epoch 48/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8913\n",
      "Epoch 00048: loss improved from 0.35641 to 0.35320, saving model to weights-improvement-48-0.3532.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3532 - acc: 0.8915\n",
      "Epoch 49/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8943\n",
      "Epoch 00049: loss improved from 0.35320 to 0.34330, saving model to weights-improvement-49-0.3433.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.3433 - acc: 0.8942\n",
      "Epoch 50/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8957\n",
      "Epoch 00050: loss improved from 0.34330 to 0.34149, saving model to weights-improvement-50-0.3415.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3415 - acc: 0.8958\n",
      "Epoch 51/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.9021\n",
      "Epoch 00051: loss improved from 0.34149 to 0.32242, saving model to weights-improvement-51-0.3224.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3224 - acc: 0.9021\n",
      "Epoch 52/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9058\n",
      "Epoch 00052: loss improved from 0.32242 to 0.30954, saving model to weights-improvement-52-0.3095.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3095 - acc: 0.9058\n",
      "Epoch 53/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9082\n",
      "Epoch 00053: loss improved from 0.30954 to 0.30067, saving model to weights-improvement-53-0.3007.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.3007 - acc: 0.9083\n",
      "Epoch 54/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9032\n",
      "Epoch 00054: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.3089 - acc: 0.9031\n",
      "Epoch 55/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9052\n",
      "Epoch 00055: loss did not improve\n",
      "18849/18849 [==============================] - 53s 3ms/step - loss: 0.3106 - acc: 0.9052\n",
      "Epoch 56/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9153\n",
      "Epoch 00056: loss improved from 0.30067 to 0.28075, saving model to weights-improvement-56-0.2807.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2807 - acc: 0.9152\n",
      "Epoch 57/150\n",
      "18800/18849 [============================>.] - ETA: 4s - loss: 0.2867 - acc: 0.9107\n",
      "Epoch 00057: loss did not improve\n",
      "18849/18849 [==============================] - 1791s 95ms/step - loss: 0.2872 - acc: 0.9106\n",
      "Epoch 58/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9186\n",
      "Epoch 00058: loss improved from 0.28075 to 0.27181, saving model to weights-improvement-58-0.2718.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2718 - acc: 0.9186\n",
      "Epoch 59/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9172\n",
      "Epoch 00059: loss improved from 0.27181 to 0.27052, saving model to weights-improvement-59-0.2705.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2705 - acc: 0.9172\n",
      "Epoch 60/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9161\n",
      "Epoch 00060: loss improved from 0.27052 to 0.26912, saving model to weights-improvement-60-0.2691.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2691 - acc: 0.9161\n",
      "Epoch 61/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9176\n",
      "Epoch 00061: loss improved from 0.26912 to 0.26052, saving model to weights-improvement-61-0.2605.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.2605 - acc: 0.9174\n",
      "Epoch 62/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9210\n",
      "Epoch 00062: loss improved from 0.26052 to 0.25272, saving model to weights-improvement-62-0.2527.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.2527 - acc: 0.9211\n",
      "Epoch 63/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9183\n",
      "Epoch 00063: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2565 - acc: 0.9184\n",
      "Epoch 64/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9211\n",
      "Epoch 00064: loss improved from 0.25272 to 0.24826, saving model to weights-improvement-64-0.2483.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2483 - acc: 0.9211\n",
      "Epoch 65/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9268\n",
      "Epoch 00065: loss improved from 0.24826 to 0.23578, saving model to weights-improvement-65-0.2358.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2358 - acc: 0.9267\n",
      "Epoch 66/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9289\n",
      "Epoch 00066: loss improved from 0.23578 to 0.22885, saving model to weights-improvement-66-0.2289.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2289 - acc: 0.9290\n",
      "Epoch 67/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9273\n",
      "Epoch 00067: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2304 - acc: 0.9271\n",
      "Epoch 68/150\n",
      "18800/18849 [============================>.] - ETA: 5:07 - loss: 0.2391 - acc: 0.9263 \n",
      "Epoch 00068: loss did not improve\n",
      "18849/18849 [==============================] - 117890s 6s/step - loss: 0.2390 - acc: 0.9263\n",
      "Epoch 69/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9254\n",
      "Epoch 00069: loss did not improve\n",
      "18849/18849 [==============================] - 64s 3ms/step - loss: 0.2422 - acc: 0.9253\n",
      "Epoch 70/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9260\n",
      "Epoch 00070: loss did not improve\n",
      "18849/18849 [==============================] - 59s 3ms/step - loss: 0.2348 - acc: 0.9259\n",
      "Epoch 71/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9281\n",
      "Epoch 00071: loss improved from 0.22885 to 0.22638, saving model to weights-improvement-71-0.2264.hdf5\n",
      "18849/18849 [==============================] - 55s 3ms/step - loss: 0.2264 - acc: 0.9278\n",
      "Epoch 72/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9327\n",
      "Epoch 00072: loss improved from 0.22638 to 0.21693, saving model to weights-improvement-72-0.2169.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2169 - acc: 0.9325\n",
      "Epoch 73/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9307\n",
      "Epoch 00073: loss improved from 0.21693 to 0.21491, saving model to weights-improvement-73-0.2149.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2149 - acc: 0.9308\n",
      "Epoch 74/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9232\n",
      "Epoch 00074: loss did not improve\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.2316 - acc: 0.9231\n",
      "Epoch 75/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9350\n",
      "Epoch 00075: loss improved from 0.21491 to 0.21480, saving model to weights-improvement-75-0.2148.hdf5\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.2148 - acc: 0.9351\n",
      "Epoch 76/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9402\n",
      "Epoch 00076: loss improved from 0.21480 to 0.19832, saving model to weights-improvement-76-0.1983.hdf5\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.1983 - acc: 0.9400\n",
      "Epoch 77/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9387\n",
      "Epoch 00077: loss improved from 0.19832 to 0.19102, saving model to weights-improvement-77-0.1910.hdf5\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.1910 - acc: 0.9387\n",
      "Epoch 78/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9386\n",
      "Epoch 00078: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1958 - acc: 0.9386\n",
      "Epoch 79/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9263\n",
      "Epoch 00079: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2318 - acc: 0.9264\n",
      "Epoch 80/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9271\n",
      "Epoch 00080: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2298 - acc: 0.9272\n",
      "Epoch 81/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9374\n",
      "Epoch 00081: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.2033 - acc: 0.9373\n",
      "Epoch 82/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9341\n",
      "Epoch 00082: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2080 - acc: 0.9341\n",
      "Epoch 83/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9392\n",
      "Epoch 00083: loss improved from 0.19102 to 0.19057, saving model to weights-improvement-83-0.1906.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1906 - acc: 0.9392\n",
      "Epoch 84/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9424\n",
      "Epoch 00084: loss improved from 0.19057 to 0.18414, saving model to weights-improvement-84-0.1841.hdf5\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.1841 - acc: 0.9425\n",
      "Epoch 85/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9412\n",
      "Epoch 00085: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1872 - acc: 0.9412\n",
      "Epoch 86/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9347\n",
      "Epoch 00086: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.2021 - acc: 0.9347\n",
      "Epoch 87/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9377\n",
      "Epoch 00087: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1943 - acc: 0.9377\n",
      "Epoch 88/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9432\n",
      "Epoch 00088: loss improved from 0.18414 to 0.17923, saving model to weights-improvement-88-0.1792.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1792 - acc: 0.9432\n",
      "Epoch 89/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9493\n",
      "Epoch 00089: loss improved from 0.17923 to 0.16534, saving model to weights-improvement-89-0.1653.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1653 - acc: 0.9492\n",
      "Epoch 90/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9349\n",
      "Epoch 00090: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1967 - acc: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9368\n",
      "Epoch 00091: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1960 - acc: 0.9367\n",
      "Epoch 92/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9389\n",
      "Epoch 00092: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1844 - acc: 0.9387\n",
      "Epoch 93/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9410\n",
      "Epoch 00093: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1852 - acc: 0.9410\n",
      "Epoch 94/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9390\n",
      "Epoch 00094: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1840 - acc: 0.9389\n",
      "Epoch 95/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9411\n",
      "Epoch 00095: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1827 - acc: 0.9412\n",
      "Epoch 96/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9447\n",
      "Epoch 00096: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1741 - acc: 0.9448\n",
      "Epoch 97/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.9295 - acc: 0.5987\n",
      "Epoch 00097: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 1.9300 - acc: 0.5983\n",
      "Epoch 98/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.5703 - acc: 0.5902\n",
      "Epoch 00098: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 1.5737 - acc: 0.5897\n",
      "Epoch 99/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 1.5714 - acc: 0.5856\n",
      "Epoch 00099: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 1.5705 - acc: 0.5857\n",
      "Epoch 100/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.9637 - acc: 0.6981\n",
      "Epoch 00100: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.9631 - acc: 0.6984\n",
      "Epoch 101/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.7052 - acc: 0.7738\n",
      "Epoch 00101: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.7051 - acc: 0.7738\n",
      "Epoch 102/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8191\n",
      "Epoch 00102: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.5390 - acc: 0.8188\n",
      "Epoch 103/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8659\n",
      "Epoch 00103: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.4091 - acc: 0.8658\n",
      "Epoch 104/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8955\n",
      "Epoch 00104: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.3283 - acc: 0.8954\n",
      "Epoch 105/150\n",
      "18800/18849 [============================>.] - ETA: 28s - loss: 0.2790 - acc: 0.9111\n",
      "Epoch 00105: loss did not improve\n",
      "18849/18849 [==============================] - 10852s 576ms/step - loss: 0.2789 - acc: 0.9111\n",
      "Epoch 106/150\n",
      "18800/18849 [============================>.] - ETA: 56s - loss: 0.2443 - acc: 0.9260 \n",
      "Epoch 00106: loss did not improve\n",
      "18849/18849 [==============================] - 21855s 1s/step - loss: 0.2443 - acc: 0.9259\n",
      "Epoch 107/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9343\n",
      "Epoch 00107: loss did not improve\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.2140 - acc: 0.9343\n",
      "Epoch 108/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9332\n",
      "Epoch 00108: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2127 - acc: 0.9333\n",
      "Epoch 109/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9305\n",
      "Epoch 00109: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2220 - acc: 0.9307\n",
      "Epoch 110/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9421\n",
      "Epoch 00110: loss did not improve\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1956 - acc: 0.9421\n",
      "Epoch 111/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9395\n",
      "Epoch 00111: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1939 - acc: 0.9394\n",
      "Epoch 112/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9372\n",
      "Epoch 00112: loss did not improve\n",
      "18849/18849 [==============================] - 56s 3ms/step - loss: 0.2030 - acc: 0.9373\n",
      "Epoch 113/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9335\n",
      "Epoch 00113: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.2071 - acc: 0.9336\n",
      "Epoch 114/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9317\n",
      "Epoch 00114: loss did not improve\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.2106 - acc: 0.9315\n",
      "Epoch 115/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9361\n",
      "Epoch 00115: loss did not improve\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.2007 - acc: 0.9362\n",
      "Epoch 116/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9406\n",
      "Epoch 00116: loss did not improve\n",
      "18849/18849 [==============================] - 52s 3ms/step - loss: 0.1908 - acc: 0.9405\n",
      "Epoch 117/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9401\n",
      "Epoch 00117: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1906 - acc: 0.9402\n",
      "Epoch 118/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9427\n",
      "Epoch 00118: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1841 - acc: 0.9428\n",
      "Epoch 119/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9444\n",
      "Epoch 00119: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1798 - acc: 0.9442\n",
      "Epoch 120/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9460\n",
      "Epoch 00120: loss did not improve\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1741 - acc: 0.9462\n",
      "Epoch 121/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9457\n",
      "Epoch 00121: loss did not improve\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1677 - acc: 0.9457\n",
      "Epoch 122/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9441\n",
      "Epoch 00122: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1731 - acc: 0.9441\n",
      "Epoch 123/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9526\n",
      "Epoch 00123: loss improved from 0.16534 to 0.15367, saving model to weights-improvement-123-0.1537.hdf5\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1537 - acc: 0.9525\n",
      "Epoch 124/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9519\n",
      "Epoch 00124: loss improved from 0.15367 to 0.15294, saving model to weights-improvement-124-0.1529.hdf5\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1529 - acc: 0.9518\n",
      "Epoch 125/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9465\n",
      "Epoch 00125: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1653 - acc: 0.9466\n",
      "Epoch 126/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9429\n",
      "Epoch 00126: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1753 - acc: 0.9429\n",
      "Epoch 127/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9483\n",
      "Epoch 00127: loss did not improve\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1663 - acc: 0.9482\n",
      "Epoch 128/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9439\n",
      "Epoch 00128: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1700 - acc: 0.9440\n",
      "Epoch 129/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9516\n",
      "Epoch 00129: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1546 - acc: 0.9515\n",
      "Epoch 130/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9546\n",
      "Epoch 00130: loss improved from 0.15294 to 0.14639, saving model to weights-improvement-130-0.1464.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1464 - acc: 0.9546\n",
      "Epoch 131/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9568\n",
      "Epoch 00131: loss improved from 0.14639 to 0.13801, saving model to weights-improvement-131-0.1380.hdf5\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1380 - acc: 0.9568\n",
      "Epoch 132/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9528\n",
      "Epoch 00132: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1488 - acc: 0.9527\n",
      "Epoch 133/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9562\n",
      "Epoch 00133: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1433 - acc: 0.9561\n",
      "Epoch 134/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9514\n",
      "Epoch 00134: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1542 - acc: 0.9512\n",
      "Epoch 135/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9462\n",
      "Epoch 00135: loss did not improve\n",
      "18849/18849 [==============================] - 52s 3ms/step - loss: 0.1641 - acc: 0.9462\n",
      "Epoch 136/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9474\n",
      "Epoch 00136: loss did not improve\n",
      "18849/18849 [==============================] - 49s 3ms/step - loss: 0.1581 - acc: 0.9474\n",
      "Epoch 137/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9598\n",
      "Epoch 00137: loss improved from 0.13801 to 0.13076, saving model to weights-improvement-137-0.1308.hdf5\n",
      "18849/18849 [==============================] - 57s 3ms/step - loss: 0.1308 - acc: 0.9598\n",
      "Epoch 138/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9589\n",
      "Epoch 00138: loss did not improve\n",
      "18849/18849 [==============================] - 59s 3ms/step - loss: 0.1320 - acc: 0.9589\n",
      "Epoch 139/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9527\n",
      "Epoch 00139: loss did not improve\n",
      "18849/18849 [==============================] - 55s 3ms/step - loss: 0.1485 - acc: 0.9527\n",
      "Epoch 140/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9578\n",
      "Epoch 00140: loss did not improve\n",
      "18849/18849 [==============================] - 53s 3ms/step - loss: 0.1370 - acc: 0.9577\n",
      "Epoch 141/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9570\n",
      "Epoch 00141: loss did not improve\n",
      "18849/18849 [==============================] - 53s 3ms/step - loss: 0.1378 - acc: 0.9569\n",
      "Epoch 142/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9523\n",
      "Epoch 00142: loss did not improve\n",
      "18849/18849 [==============================] - 54s 3ms/step - loss: 0.1479 - acc: 0.9524\n",
      "Epoch 143/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9582\n",
      "Epoch 00143: loss did not improve\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.1321 - acc: 0.9580\n",
      "Epoch 144/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9551\n",
      "Epoch 00144: loss did not improve\n",
      "18849/18849 [==============================] - 51s 3ms/step - loss: 0.1408 - acc: 0.9551\n",
      "Epoch 145/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9616\n",
      "Epoch 00145: loss improved from 0.13076 to 0.12597, saving model to weights-improvement-145-0.1260.hdf5\n",
      "18849/18849 [==============================] - 50s 3ms/step - loss: 0.1260 - acc: 0.9617\n",
      "Epoch 146/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9593\n",
      "Epoch 00146: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1306 - acc: 0.9592\n",
      "Epoch 147/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9511\n",
      "Epoch 00147: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1472 - acc: 0.9511\n",
      "Epoch 148/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9550\n",
      "Epoch 00148: loss did not improve\n",
      "18849/18849 [==============================] - 47s 3ms/step - loss: 0.1438 - acc: 0.9551\n",
      "Epoch 149/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9576\n",
      "Epoch 00149: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1361 - acc: 0.9577\n",
      "Epoch 150/150\n",
      "18800/18849 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9560\n",
      "Epoch 00150: loss did not improve\n",
      "18849/18849 [==============================] - 48s 3ms/step - loss: 0.1337 - acc: 0.9560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1818d75748>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(final_X, final_y, epochs=150, batch_size=50, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-145-0.1260.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "weight_mat = []\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    weight_mat.append(weights)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pick a random sequence from the input to begin with\n",
    "rand = random.randint(0, len(final_X))\n",
    "pattern = final_X[rand]\n",
    "\n",
    "# grab the indices of the\n",
    "nums = ([np.argmax(x) for x in pattern])\n",
    "starter = (''.join([num_to_char[value] for value in nums]))\n",
    "f = open(\"A.txt\", \"w+\")\n",
    "f.write(starter)\n",
    "f.close()\n",
    "for i in range (1000):\n",
    "    # Making sure we have the right size\n",
    "    to_predict = np.reshape(pattern[-40:], (1, 40, 61))\n",
    "    prediction = model.predict(to_predict, verbose=0)\n",
    "    #Getting index of largest element\n",
    "    index = np.argmax(prediction)\n",
    "    #Changing to categorical because that is how 'pattern' elements are stored\n",
    "    y = keras.utils.to_categorical(index, num_classes=61)\n",
    "    pattern = np.vstack((pattern, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" deeds of youth,\\nSo I, made lame by Fortuon and sind.\\nA me say me thut sake the eive thee mo thee mure ble?\\nWhet I thie thy thy sweet bellow stay she\\nThe ulose that liss your the coproune:\\n  Or and my fore the self sell-burte.\\nWhen I hove when the sagh thee mad in theil.\\nWho  love aw ense hour all this bead you,\\nWhich should time's faie, thou that I doof merther see,\\nAnd reauby like in my mankeds be wady sack,\\nBue old not shour, for thy sume oncerisw.\\nAnd to have to not the sweet thy gunthound,\\nAnd all thy make, not more poour ant be.\\nOf their sell berumy sand,\\nAnd love as faist to bead to the loves of all thee,\\nWhece abss merss aw faie, now with mestor carlouly,\\nWhot worth their pore this lave al mends,\\nThat you that I deam mo tomtrunglesssught,\\nAnd you prominit thy form mu to thoughts'sell.\\nWhet borimy im hour thee not sammer's see,\\nAnd urifhind on and me farthes is dee,\\nBeest thee shour, and I her the gunted your self,\\nSur whon to heart that with have yourus blad,\\nAnd doring thy ficmerung ou day ere sught,\\nAnd beauty's th\", '|')\n"
     ]
    }
   ],
   "source": [
    "nums = ([np.argmax(x) for x in pattern])\n",
    "string = [num_to_char[value] for value in nums]\n",
    "\n",
    "final = (''.join(string), \"|\")\n",
    "print (final)\n",
    "f = open(\"A.txt\", \"w+\")\n",
    "f.write(final[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-145-0.1260.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "temp = 1.0\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    for w in weights:\n",
    "        w *= temp\n",
    "    layer.set_weights(weights)\n",
    "    \n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mat2 = []\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    weight_mat2.append(weights)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seed = \"shall I compare thee to a summer's day?\\n\"\n",
    "starter = []\n",
    "for character in init_seed:\n",
    "    num = char_to_num[character]\n",
    "    starter.append(num)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "\" shall I compare thee to a summer's day?\n",
      " \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 40 into shape (1,40,61)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-7e5bac7ebaa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m440\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m61.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    230\u001b[0m            [5, 6]])\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 40 into shape (1,40,61)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#start = np.random.randint(0, len(newX) - 1)\n",
    "start_point = starter\n",
    "\n",
    "print (len(start_point))\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in start_point]), \"\\\"\")\n",
    "f = open(\"poem4.txt\", \"w+\")\n",
    "f.write(init_seed)\n",
    "\n",
    "for i in range(440):\n",
    "    x = np.reshape(start_point, (1, len(start_point), 1))\n",
    "    x = x / 61.0\n",
    "    prediction = model.predict(x)\n",
    "    index = np.argmax(prediction)\n",
    "    print(index)\n",
    "    res = int_map_to_char[index]\n",
    "    print(res)\n",
    "    line = [int_map_to_char[val] for val in start_point]\n",
    "    f.write(res)\n",
    "    start_point.append(index)\n",
    "    start_point = start_point[1:len(start_point)]\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poem = \"shall I compare thee to a summer's day?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "poem = \"shall I compare thee to a summer's day?\\n\"\n",
    "\n",
    "poem_ints = []\n",
    "for char in poem:\n",
    "    poem_ints.append(char_map_to_int[char])\n",
    "poem_ints = np.asarray(poem_ints)\n",
    "poem_floats = poem_ints / 39.0\n",
    "\n",
    "f = open(\"poem5.txt\", \"w+\")\n",
    "\n",
    "for i in range(440):\n",
    "    g = np.reshape(poem_floats, (1, len(poem_floats), 1))\n",
    "    p = model.predict(g)\n",
    "    new_int = np.argmax(p)\n",
    "    poem_ints = np.append(poem_ints, new_int)\n",
    "    \n",
    "    new_char = int_map_to_char[new_int]\n",
    "    poem += new_char\n",
    "    \n",
    "    new_float = new_int / 39.0\n",
    "    poem_floats = poem_floats[1:40]\n",
    "    poem_floats = np.append(poem_floats, new_float)\n",
    "    print(len(poem_floats))\n",
    "    \n",
    "    \n",
    "f.write(poem)   \n",
    "f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ -1.05907870e-02,   2.67996341e-01,  -5.28563440e-01,\n",
       "            8.05161968e-02,   2.47330293e-01,   2.44568825e-01,\n",
       "            3.55412543e-01,   1.55443633e-02,   1.14183629e+00,\n",
       "            2.80385017e-01,   3.01117957e-01,   4.88379300e-01,\n",
       "            1.34556115e-01,  -1.56819390e-03,  -4.33472432e-02,\n",
       "            3.04603398e-01,  -1.75222889e-01,   7.99145103e-02,\n",
       "            2.83047736e-01,  -8.49842802e-02,   6.65172487e-02,\n",
       "           -9.19732079e-02,   5.19059896e-02,   1.02073833e-01,\n",
       "           -1.94024518e-01,   7.37666711e-02,   3.94382954e-01,\n",
       "           -6.98201880e-02,  -4.84348685e-02,   1.02403931e-01,\n",
       "            4.12248075e-02,   5.72365895e-02,   1.00207198e-02,\n",
       "           -2.27203712e-01,  -8.01381052e-01,  -1.68690577e-01,\n",
       "            3.60160507e-02,   5.43787740e-02,  -4.87413913e-01,\n",
       "           -3.34511697e-02,   3.08317631e-01,   5.66044152e-01,\n",
       "            1.46033391e-01,  -7.67399907e-01,   1.32038116e-01,\n",
       "           -2.33801305e-01,   1.96718499e-01,  -3.73268366e-01,\n",
       "            2.32708361e-02,   2.73267478e-01,  -2.70345192e-02,\n",
       "            2.91575551e-01,   1.00426031e-02,  -1.03673004e-01,\n",
       "           -3.18259776e-01,  -2.70933717e-01,  -2.14439668e-02,\n",
       "            8.11962560e-02,   5.99291742e-01,   4.23090100e-01,\n",
       "            1.50352865e-01,   3.08942460e-02,   8.34563151e-02,\n",
       "            7.55581409e-02,   1.09798586e+00,   4.95857865e-01,\n",
       "           -8.46081302e-02,   4.23514917e-02,   2.24209353e-01,\n",
       "            1.08813286e-01,  -4.66076881e-02,   4.13177907e-01,\n",
       "           -6.31603552e-03,   1.14676349e-01,   7.86074340e-01,\n",
       "           -4.01168736e-03,   1.74438417e-01,   2.21564993e-01,\n",
       "            7.46495187e-01,   2.63002694e-01,   6.48985505e-02,\n",
       "           -7.87320510e-02,  -1.57754682e-02,   2.84565389e-01,\n",
       "           -5.90071604e-02,  -1.16601996e-01,  -1.79141253e-01,\n",
       "            2.89766490e-02,  -6.37106717e-01,   4.22490448e-01,\n",
       "            1.56926528e-01,   7.97189415e-01,  -1.23969354e-01,\n",
       "            2.86594704e-02,   2.65091270e-01,   2.38727227e-01,\n",
       "           -3.97823425e-03,   1.49827421e-01,  -2.00029939e-01,\n",
       "            1.80483341e-01,   7.63598010e-02,   5.89374661e-01,\n",
       "            1.66432168e-02,  -5.50911762e-02,  -3.73070315e-02,\n",
       "            1.30610317e-01,   7.70198181e-02,  -1.74240917e-02,\n",
       "            1.29418090e-01,   3.18244487e-01,  -5.78698143e-02,\n",
       "            6.79763108e-02,  -1.11370765e-01,  -1.38577014e-01,\n",
       "           -6.23155832e-02,  -1.42146170e-01,   1.74967244e-01,\n",
       "            3.39582592e-01,   1.21050858e+00,  -7.89324641e-02,\n",
       "           -3.72535408e-01,  -3.11511546e-01,   1.80207476e-01,\n",
       "           -1.12766065e-02,  -2.36008689e-01,  -1.04748420e-01,\n",
       "            1.99846532e-02,  -3.87987673e-01,   9.94014889e-02,\n",
       "            5.00610732e-02,  -2.70065159e-01,  -7.53329992e-02,\n",
       "            3.06493398e-02,   5.06674826e-01,   1.65062815e-01,\n",
       "            8.66643041e-02,  -1.61899384e-02,  -2.53851153e-02,\n",
       "           -8.07441995e-02,   1.78529173e-01,   2.03449409e-02,\n",
       "           -1.12394221e-01,  -4.02418017e-01,   2.29763635e-03,\n",
       "            3.12943727e-01,   9.46571771e-03,   2.26698264e-01,\n",
       "            3.42149585e-02,   4.62014265e-02,  -1.43280789e-01,\n",
       "           -8.32692534e-02,  -3.16209742e-03,   2.25859389e-01,\n",
       "            1.12421103e-01,   1.02076277e-01,  -3.32213461e-01,\n",
       "           -3.69497165e-02,  -1.04153436e-02,   1.10361695e+00,\n",
       "            1.74631864e-01,  -1.83327124e-01,   3.01900119e-01,\n",
       "            2.90992744e-02,   5.53348422e-01,   2.52085738e-02,\n",
       "            5.74374460e-02,   1.66080877e-01,  -3.86155918e-02,\n",
       "           -1.81520730e-02,   5.86954355e-01,   2.03876808e-01,\n",
       "            4.44961309e-01,   2.66208708e-01,  -1.01508923e-01,\n",
       "            6.28320053e-02,   9.91252184e-01,   3.02441180e-01,\n",
       "           -3.20063457e-02,  -8.18718821e-02,  -1.54209912e-01,\n",
       "            5.87539256e-01,   8.38586092e-01,   4.96091247e-01,\n",
       "           -8.41347724e-02,  -1.05259102e-02,  -2.99684376e-01,\n",
       "           -1.46819636e-01,   5.87889791e-01,  -1.14745602e-01,\n",
       "            1.93009153e-03,   7.18677759e-01,  -1.32030398e-01,\n",
       "            1.88240916e-01,  -1.62942901e-01,   1.32438466e-01,\n",
       "           -3.48637372e-01,  -9.78437141e-02,   1.17917266e-02,\n",
       "            7.43729100e-02,  -1.99911520e-01,   8.89329314e-02,\n",
       "            1.62026644e-01,   1.39309001e+00,  -1.99559554e-01,\n",
       "            4.65213507e-01,   4.20955867e-02,   7.05273822e-02,\n",
       "           -8.73577520e-02,   6.36394620e-02,   6.91278279e-02,\n",
       "           -5.57986081e-01,   2.68206030e-01,   1.55322373e-01,\n",
       "           -1.49887770e-01,   7.19588622e-02,   1.49616063e-01,\n",
       "            1.04965501e-01,   1.26569018e-01,   7.24525079e-02,\n",
       "            1.59093335e-01,   9.51676145e-02,  -4.57226187e-01,\n",
       "            3.75467949e-02,   3.73704880e-02,   4.65426654e-01,\n",
       "            1.45524621e-01,   1.26831740e-01,   2.45593917e-02,\n",
       "            1.99002281e-01,   1.16385054e-02,   5.26241243e-01,\n",
       "            1.53417930e-01,   1.20326027e-01,  -2.83439398e-01,\n",
       "           -2.69563735e-01,  -8.02087188e-02,   1.69082642e-01,\n",
       "            1.59749836e-01,   1.60141563e+00,   8.63067899e-03,\n",
       "            3.08687948e-02,   2.86439240e-01,   5.22226989e-02,\n",
       "            8.63684535e-01,  -1.18196316e-01,  -6.72130212e-02,\n",
       "            1.57066491e-02,  -6.23360395e-01,   4.77145724e-02,\n",
       "            1.24199331e-01,   6.17055357e-01,   1.11382127e-01,\n",
       "           -9.18072686e-02,  -8.72101188e-02,   2.99587637e-01,\n",
       "            1.61982402e-01,  -3.42838541e-02,   3.80030841e-01,\n",
       "            2.09622830e-01,  -6.07498586e-02,   1.01693124e-01,\n",
       "            1.84616640e-01,   2.55191792e-02,  -8.58474057e-03,\n",
       "           -6.16313890e-02,  -5.35885394e-01,  -4.12912369e-02,\n",
       "            4.16706726e-02,  -1.39993921e-01,   1.69565842e-01,\n",
       "            7.37894118e-01,  -4.79016677e-02,   2.00863749e-01,\n",
       "           -1.06034465e-02,   1.60914838e-01,   1.24247752e-01,\n",
       "            2.31732175e-01,  -1.66376382e-02,  -1.90544009e-01,\n",
       "            3.18797603e-02,   6.16834685e-02,   1.80537909e-01,\n",
       "           -2.70895720e-01,   6.93190694e-02,  -2.94152964e-02,\n",
       "           -1.01494633e-01,   1.47446841e-02,   7.28099719e-02,\n",
       "           -5.74292064e-01,  -5.85060902e-02,   9.48291458e-03,\n",
       "           -1.66381046e-01,   1.74151845e-02,   8.06053281e-02,\n",
       "           -2.59769037e-02,   2.83147186e-01,  -4.65683937e-02,\n",
       "           -1.12660296e-01,  -9.71195549e-02,   1.10615388e-01,\n",
       "           -7.83175007e-02,   1.38977673e-02,  -1.29432514e-01,\n",
       "            1.21821299e-01,  -1.69903472e-01,  -1.81749221e-02,\n",
       "            7.17379376e-02,   1.28886744e-01,  -1.26866475e-01,\n",
       "            1.13380261e-01,  -3.49194714e-04,   2.30462737e-02,\n",
       "            1.69814646e-01,   2.36916944e-01,   1.76068270e+00,\n",
       "           -1.13720186e-02,  -6.23063780e-02,   2.67957211e-01,\n",
       "           -8.02905798e-01,   7.55021200e-02,   1.18519619e-01,\n",
       "            2.06729129e-01,   8.33468437e-02,   1.35651037e-01,\n",
       "            1.12569429e-01,   4.43887198e-03,   1.57242149e-01,\n",
       "            3.36864620e-01,  -4.77742292e-02,   1.57844916e-01,\n",
       "           -2.84218997e-01,  -3.94109525e-02,   7.73613602e-02,\n",
       "           -4.87763956e-02,   1.17696784e-01,   4.16645631e-02,\n",
       "            1.86598804e-02,   5.24628721e-02,   1.15424357e-01,\n",
       "            7.73451746e-01,   1.31467611e-01,  -1.26622677e-01,\n",
       "            7.15863630e-02,   1.22532882e-01,  -3.03700238e-01,\n",
       "            1.76852286e-01,   8.42120051e-02,   1.13184668e-01,\n",
       "            5.84788807e-02,  -9.95857920e-03,   3.10429096e-01,\n",
       "            4.38564494e-02,   2.53776371e-01,  -1.28772333e-01,\n",
       "            1.41334593e-01,  -1.38230011e-01,   1.71523362e-01,\n",
       "           -1.44256219e-01,  -2.82389134e-01,  -1.09937571e-01,\n",
       "            2.00733155e-01,   2.56162975e-02,  -7.17207789e-02,\n",
       "           -4.44813184e-02,  -2.42054522e-01,   1.42101929e-01,\n",
       "           -1.46980071e-02,   4.34432402e-02,   1.63107514e-02,\n",
       "            1.43070534e-01,   1.16502576e-01,   4.36921138e-03,\n",
       "           -5.93282431e-02,   2.04232407e+00,  -1.08493872e-01,\n",
       "           -5.10302365e-01,  -8.70622993e-02,   9.26850513e-02,\n",
       "           -1.83116645e-02,  -2.99081892e-01,   4.18586791e-01,\n",
       "            1.55606702e-01,   4.40505028e-01,  -1.65781565e-02,\n",
       "            1.07811950e-01,  -1.25551760e-01,  -8.98756534e-02,\n",
       "            1.14165716e-01,  -4.60950844e-02,  -3.21546979e-02,\n",
       "            4.62677106e-02,   1.51371822e-01,   9.46081057e-02,\n",
       "           -3.78496833e-02,   1.24587402e-01,  -2.32699104e-02,\n",
       "            5.94104268e-02,   1.40602911e+00,   4.77764197e-03,\n",
       "            4.78208661e-02,  -1.33889288e-01,   1.61967292e-01,\n",
       "            2.06315446e+00,  -6.41788602e-01,   4.28283691e-01,\n",
       "           -3.63361627e-01,  -1.01283622e+00,   1.05225635e+00,\n",
       "            8.86414587e-01,   1.85387120e-01,  -1.59543380e-01,\n",
       "           -1.75669685e-03,   9.43827555e-02,   5.45005687e-02,\n",
       "            1.70253277e-01,   1.16242923e-01,  -2.39493344e-02,\n",
       "           -9.62827876e-02,  -8.07450637e-02,   5.71199022e-02,\n",
       "            2.21259724e-02,   7.78301001e-01,  -2.21725088e-02,\n",
       "            8.67360383e-02,  -1.43610593e-03,  -3.02145984e-02,\n",
       "           -4.88216802e-03,  -1.52699620e-01,   1.06988698e-01,\n",
       "           -6.75350353e-02,   2.90358156e-01,  -7.49286413e-02,\n",
       "           -9.70322266e-03,   4.72308338e-01,   7.09963858e-01,\n",
       "            3.17613967e-02,  -4.08489406e-02,   1.28443569e-01,\n",
       "           -1.64861965e+00,  -7.39547834e-02,   2.26022482e-01,\n",
       "            1.08109266e-01,   6.86348649e-03,  -1.46057785e+00,\n",
       "            1.78058803e+00,  -5.76005913e-02,  -1.33643702e-01,\n",
       "           -4.71590698e-01,   1.97808146e-02,   5.27422547e-01,\n",
       "           -5.03089428e-01,   5.87716959e-02,  -4.29469943e-02,\n",
       "            1.02542508e-02,  -1.43806368e-01,  -1.07023120e-01,\n",
       "           -3.16238664e-02,   4.86994296e-01,  -4.89406824e-01,\n",
       "           -1.27310514e-01,  -8.33773520e-03,   5.22103868e-02,\n",
       "           -1.09906897e-01,  -9.38726962e-02,  -6.83604002e-01,\n",
       "           -6.45343423e-01,  -1.37261758e-02,  -1.03567176e-01,\n",
       "           -4.90395069e-01,   7.68399844e-03,   1.18411124e+00,\n",
       "            2.51867414e-01,  -1.65469125e-02,  -9.75914523e-02,\n",
       "            5.92436492e-01,  -2.09464356e-02,   4.21134979e-02,\n",
       "           -9.00359675e-02,   1.45254946e+00,  -1.59155503e-02,\n",
       "            6.21646568e-02,  -3.23592126e-02,   1.11057878e+00,\n",
       "           -1.85860977e-01,  -5.32809384e-02,   4.14092481e-01,\n",
       "            3.58948894e-02,  -2.47507226e-02,   1.04639030e+00,\n",
       "            6.99863657e-02,  -3.20295393e-02,   3.18531483e-01,\n",
       "           -5.82601056e-02,  -3.68561186e-02,   4.40179706e-02,\n",
       "            1.01799035e+00,  -1.15770452e-01,  -7.12151006e-02,\n",
       "            1.34700239e-01,  -1.60944000e-01,   8.35011750e-02,\n",
       "           -2.47371212e-01,   3.45962420e-02,  -1.26442224e-01,\n",
       "           -5.48335984e-02,  -1.80819146e-02,   1.04829611e-03,\n",
       "           -8.50886554e-02,  -2.51208078e-02,  -2.25525543e-01,\n",
       "           -7.36322701e-02,  -3.75970080e-02,  -6.46719933e-02,\n",
       "           -3.33085835e-01,   2.02171445e+00,  -1.50399908e-01,\n",
       "           -5.02214395e-02,  -1.23732962e-01,  -1.21092844e+00,\n",
       "            3.04394942e-02,   1.25633404e-01,   2.22078353e-01,\n",
       "            8.96834210e-02,   5.24189807e-02,  -8.28517601e-03,\n",
       "           -6.21961653e-02,   7.05512688e-02,   2.07310128e+00,\n",
       "            1.07149176e-01,  -7.51094669e-02,   2.58923960e+00,\n",
       "           -7.39182606e-02,  -8.77965987e-02,  -3.99453670e-01,\n",
       "            8.81958082e-02,  -1.10560253e-01,   3.11152055e-03,\n",
       "            1.04448833e-02,  -5.74245863e-02,   1.15378775e-01,\n",
       "           -1.98910106e-02,  -2.15301923e-02,   9.67994750e-01,\n",
       "           -2.00185869e-02,   9.22846496e-01,  -1.10736918e-02,\n",
       "            4.68530878e-02,   9.77209359e-02,   1.68810621e-01,\n",
       "            8.25300533e-03,  -5.08096337e-01,   6.76398501e-02,\n",
       "            4.44210529e-01,   1.38247514e+00,   4.65228483e-02,\n",
       "           -1.72296852e-01,  -9.11998674e-02,  -6.01809099e-02,\n",
       "           -1.53709924e+00,  -1.43426001e-01,  -5.03715873e-02,\n",
       "           -6.65153682e-01,   7.78114870e-02,   2.98070181e-02,\n",
       "           -1.96537688e-01,   1.76781869e+00,   1.06418647e-01,\n",
       "           -3.38974339e-03,   7.65642598e-02,  -3.08393799e-02,\n",
       "            2.15300471e-01,   9.96128544e-02,   1.09096408e-01,\n",
       "            2.54263878e+00,   1.13840029e-02,  -1.36506581e+00,\n",
       "           -2.05505043e-01,  -2.47710906e-02,   7.99026266e-02,\n",
       "           -3.96462381e-02,  -4.12203461e-01,  -2.54692256e-01,\n",
       "           -4.22898293e-01,   5.02641909e-02,  -6.79005310e-02,\n",
       "           -1.86978117e-01,  -8.79638195e-02,   2.41716132e-01,\n",
       "           -9.66131166e-02,  -9.25035402e-03,  -4.77357596e-01,\n",
       "            2.33783387e-02,  -1.38931200e-01,  -1.90015689e-01,\n",
       "           -1.37552544e-01,  -2.61007726e-01,  -7.71580040e-02,\n",
       "           -2.05217648e+00,   1.04377586e-02,  -1.93971302e-02,\n",
       "            6.59406856e-02,   7.09975123e-01,  -5.98557889e-01,\n",
       "            1.81306869e-01,   7.18887448e-01,   5.38838446e-01,\n",
       "            6.32866144e-01,  -4.60811257e-02,   1.47941101e+00,\n",
       "            4.71753441e-03,   1.03645906e-01,   1.57717705e+00,\n",
       "           -1.46837163e+00,  -8.64443600e-01,  -9.76825431e-02,\n",
       "            2.35007510e-01,  -9.79599535e-01,  -1.88461393e-01,\n",
       "            7.59669423e-01,  -2.86518961e-01,  -3.53594333e-01,\n",
       "           -3.00506443e-01,  -8.44478786e-01,   1.80635780e-01,\n",
       "           -1.23790801e-01,   1.03506282e-01,   9.54115748e-01,\n",
       "            3.78709547e-02,   3.27207863e-01,   4.79148418e-01,\n",
       "            6.40103281e-01,   2.98110217e-01,  -1.67113764e-03,\n",
       "           -2.35963702e-01,  -6.63843572e-01,  -9.12957549e-01,\n",
       "            5.11696376e-02,  -4.39424962e-01,   8.40607733e-02,\n",
       "            2.73760676e-01,   2.13375688e-01,   8.82947981e-01,\n",
       "           -4.44940388e-01,  -1.81512341e-01,  -6.85306564e-02,\n",
       "           -3.93787533e-01,  -1.68644786e-02,  -1.01045024e+00,\n",
       "           -2.80444950e-01,   2.72810459e-01,   4.95088816e-01,\n",
       "            3.44489872e-01,  -2.10684910e-01,  -3.54252666e-01,\n",
       "            5.13856828e-01,  -5.15603781e-01,  -2.62350917e-01,\n",
       "            3.87726933e-01,   1.26201737e+00,   1.05465901e+00,\n",
       "            3.70060414e-01,  -1.02668071e+00,  -7.85766616e-02,\n",
       "            6.45886004e-01,   1.76513875e+00,   9.09261942e-01,\n",
       "           -4.03560162e-01,  -7.00130820e-01,   4.53623861e-01,\n",
       "            4.38341498e-01,   3.43024641e-01,   3.58670682e-01,\n",
       "           -1.55827391e+00,   2.11324781e-01,   1.26573718e+00,\n",
       "           -3.19406152e-01,  -6.11078143e-01,   5.20860016e-01,\n",
       "            9.76760507e-01,   3.78036469e-01,  -2.14290574e-01,\n",
       "           -9.91102695e-01,  -8.55422318e-02,   2.10091099e-02,\n",
       "           -3.83728534e-01,  -7.09147453e-02,  -6.08511865e-01,\n",
       "           -3.42317730e-01,  -1.13314593e+00,  -1.28433079e-01,\n",
       "            1.62523448e-01,   1.28635561e+00,  -2.40438953e-01,\n",
       "           -5.80475211e-01,   4.98932123e-01,   4.39373612e-01,\n",
       "            7.24831969e-02,  -3.59411687e-01,  -1.40643215e+00,\n",
       "            2.46314093e-01,  -8.86736251e-03,   6.85756207e-01,\n",
       "           -5.83006799e-01,   2.10496262e-01,  -6.50283098e-01,\n",
       "           -3.49966884e-01,  -3.24607521e-01,   3.29068184e-01,\n",
       "           -3.28342825e-01,   5.13975263e-01,   1.37901455e-01,\n",
       "           -9.46666002e-01,  -5.79774559e-01,   5.35988927e-01,\n",
       "            1.01745762e-01,  -1.76421201e+00,   1.31897196e-01,\n",
       "            5.88009179e-01,   1.38591719e+00,  -4.05837595e-01,\n",
       "           -3.82124424e-01,  -1.52101472e-01,   8.78658295e-02,\n",
       "            4.28604126e-01,  -1.60481071e+00,   1.72601730e-01,\n",
       "            5.54212272e-01,  -4.42307025e-01,  -4.46748853e-01,\n",
       "           -1.14059854e+00,   3.10350090e-01,  -3.06246638e-01,\n",
       "            5.24874508e-01,   3.62782776e-01,   5.48978150e-01,\n",
       "            8.84967446e-02,   3.57906163e-01,  -2.79808819e-01,\n",
       "           -1.01602532e-01,   1.74731982e+00,   5.11444092e-01,\n",
       "           -8.29455316e-01,  -5.10382414e-01,  -8.34579319e-02,\n",
       "            4.95512366e-01,   4.50491384e-02,   2.61164188e-01,\n",
       "           -5.97242236e-01,  -2.42003322e-01,  -6.19887710e-01,\n",
       "           -1.32123172e-01,  -4.24986124e-01,   3.58893991e-01,\n",
       "            2.74088532e-01,  -4.94387031e-01,  -3.49280119e-01,\n",
       "           -7.15561211e-01,  -2.37451971e-01,   1.78534460e+00,\n",
       "            2.71561712e-01,   3.33802581e-01,   2.48135611e-01,\n",
       "           -4.65766102e-01,   2.41079122e-01,  -1.64023995e-01,\n",
       "            1.63559660e-01,  -4.24068213e-01,  -4.88793075e-01,\n",
       "            4.32632685e-01,   4.38437700e-01,   7.18342841e-01,\n",
       "           -2.30673984e-01,   7.47215688e-01,   3.91970187e-01,\n",
       "           -4.72803116e-01,   1.27008271e+00,   5.11218786e-01,\n",
       "            3.58945727e-01,   3.01562529e-02,  -8.50802939e-03,\n",
       "            5.35356462e-01,   1.17089045e+00,   6.87478304e-01,\n",
       "            5.40461898e-01,   1.71098173e-01,  -9.83285844e-01,\n",
       "           -1.38656259e-01,   5.59259892e-01,  -7.28076398e-01,\n",
       "           -3.76918554e-01,   5.21547616e-01,  -8.26659441e-01,\n",
       "            6.13864660e-01,  -3.99768829e-01,   9.95058641e-02,\n",
       "            9.71838534e-02,   3.27147782e-01,   4.07503247e-01,\n",
       "           -5.85552037e-01,  -1.19968820e+00]], dtype=float32),\n",
       "  array([[ 0.03828118, -0.21864279, -0.07150712, ..., -0.19422476,\n",
       "           0.0019742 ,  0.07493116],\n",
       "         [ 0.24439597, -0.07572694,  0.02318022, ...,  0.1823027 ,\n",
       "           0.05623918, -0.12046784],\n",
       "         [-0.1057131 ,  0.11146115,  0.51466709, ..., -0.07279249,\n",
       "           0.04291231,  0.00119733],\n",
       "         ..., \n",
       "         [-0.1073979 , -0.07357254, -0.2980572 , ..., -0.08167561,\n",
       "           0.09608866,  0.20235129],\n",
       "         [ 0.12317327,  0.14522001,  0.09569658, ...,  0.22545166,\n",
       "          -0.16222985, -0.071369  ],\n",
       "         [-0.08505573,  0.0449965 ,  0.05279348, ...,  0.17130649,\n",
       "          -0.04195003, -0.17140058]], dtype=float32),\n",
       "  array([ -8.95611271e-02,   1.23701356e-01,   7.33038336e-02,\n",
       "           1.19513031e-02,   1.72832370e-01,   3.52608114e-02,\n",
       "           1.19392999e-01,   1.63798094e-01,   5.57718910e-02,\n",
       "           8.66539255e-02,   1.34247437e-01,   2.21171558e-01,\n",
       "           1.26805976e-01,   8.88875872e-02,   9.39351246e-02,\n",
       "          -4.08317856e-02,  -2.70861108e-02,   7.59458467e-02,\n",
       "           3.33992653e-02,  -3.71261053e-02,   7.29252994e-02,\n",
       "           1.69596016e-01,  -2.78827567e-02,   2.34436039e-02,\n",
       "           1.64926857e-01,  -4.28513736e-02,   1.43379131e-02,\n",
       "           7.68385306e-02,  -7.29567185e-02,   6.05437942e-02,\n",
       "           6.11902587e-02,   3.52848060e-02,   6.79749995e-02,\n",
       "           2.63568684e-02,   1.06257431e-01,   2.31070127e-02,\n",
       "           2.95307115e-02,  -3.60251591e-02,   1.32982895e-01,\n",
       "           6.04637265e-02,   8.76831785e-02,   1.37112677e-01,\n",
       "           2.50157826e-02,   3.62900496e-02,   5.09771779e-02,\n",
       "           4.18177880e-02,   5.70386015e-02,   1.05902344e-01,\n",
       "          -1.08802341e-01,   1.25059709e-01,   4.21405658e-02,\n",
       "           4.15604711e-02,  -1.61183923e-02,   2.02429462e-02,\n",
       "          -2.59952825e-02,   1.21619254e-01,   1.37147740e-01,\n",
       "           1.25830784e-01,   1.03867032e-01,   1.19193621e-01,\n",
       "          -2.85500772e-02,  -3.31297480e-02,  -3.10853459e-02,\n",
       "           1.52206659e-01,   1.25952691e-01,   9.42660719e-02,\n",
       "           2.38532070e-02,  -4.73362878e-02,   1.32419303e-01,\n",
       "           3.63937244e-02,   2.13239521e-01,   9.54186022e-02,\n",
       "           1.47680834e-01,   3.36094573e-02,   7.51797408e-02,\n",
       "          -5.25592454e-03,   7.56958872e-02,   6.96892366e-02,\n",
       "           1.55464545e-01,   1.29649546e-02,  -7.72735924e-02,\n",
       "           7.44573548e-02,   4.06178951e-01,   9.24995095e-02,\n",
       "           4.84628454e-02,   3.06769628e-02,  -6.20871335e-02,\n",
       "           2.86931749e-02,   1.31317973e-01,   1.30343707e-02,\n",
       "           8.99697319e-02,   6.92639276e-02,   2.88015902e-02,\n",
       "          -5.56410253e-02,   1.07503057e-01,   1.43661126e-01,\n",
       "           2.92965379e-02,   6.76240250e-02,   1.00360304e-01,\n",
       "           6.80407435e-02,   1.27330482e-01,   9.65428054e-02,\n",
       "           1.03484662e-02,   2.24799011e-02,   6.31335005e-02,\n",
       "          -4.07018652e-03,   5.70916235e-02,  -8.12305778e-04,\n",
       "           8.24392065e-02,   1.40138902e-02,   1.23901039e-01,\n",
       "           6.75214753e-02,  -8.15929398e-02,   5.33814244e-02,\n",
       "           5.75073026e-02,   2.39974126e-01,   1.08453229e-01,\n",
       "           1.31134346e-01,   1.23436272e-01,  -5.58105782e-02,\n",
       "           1.29606262e-01,  -2.44440455e-02,  -1.42787294e-02,\n",
       "           4.96950559e-02,   4.74088918e-03,  -6.45995960e-02,\n",
       "          -1.06017916e-02,   8.07772130e-02,   7.66801834e-02,\n",
       "          -5.59658557e-02,   1.40766487e-01,  -1.03651680e-01,\n",
       "           1.25006542e-01,   2.39543170e-02,   9.76679325e-02,\n",
       "           3.51982936e-02,   4.84380089e-02,  -4.60606702e-02,\n",
       "          -4.89136316e-02,   1.57857299e-01,  -4.43141870e-02,\n",
       "           8.10630545e-02,   4.33027782e-02,  -7.85019621e-02,\n",
       "           6.10618964e-02,   1.32870134e-02,   6.45465255e-02,\n",
       "           1.54362291e-01,   3.18854526e-02,  -3.17853168e-02,\n",
       "           6.99487999e-02,   1.61163893e-03,   1.29957989e-01,\n",
       "           1.15719341e-01,   5.86302280e-02,  -1.79195758e-02,\n",
       "          -6.11094274e-02,   1.20922431e-01,   1.12005405e-01,\n",
       "           1.62182242e-01,   4.48091254e-02,   4.44154665e-02,\n",
       "           4.07697782e-02,   1.69012517e-01,  -9.03539062e-02,\n",
       "           9.70078260e-02,   1.40867196e-02,   1.12541243e-01,\n",
       "           8.10555890e-02,   1.14639848e-02,   1.20495848e-01,\n",
       "           2.02650297e-02,   6.79839030e-02,   3.38161215e-02,\n",
       "           7.00014979e-02,   3.79831269e-02,   7.38559244e-03,\n",
       "           3.58458161e-02,  -7.98217356e-02,   1.54617921e-01,\n",
       "           1.66070089e-01,   6.60678595e-02,   1.40608195e-02,\n",
       "          -6.77501783e-02,  -1.25474095e-01,   1.58202991e-01,\n",
       "          -1.09179644e-02,  -1.30232386e-02,   1.73217449e-02,\n",
       "           1.86475381e-01,   8.66340473e-02,   9.10636038e-02,\n",
       "           9.48885828e-02,   1.75326347e-01,   2.04429239e-01,\n",
       "           1.04410738e-01,  -1.13068745e-01,  -2.78484840e-02,\n",
       "           1.25048116e-01,   4.23390195e-02,   1.06117260e+00,\n",
       "           1.04365933e+00,   8.55948448e-01,   9.79491651e-01,\n",
       "           7.95068800e-01,   1.01055443e+00,   8.43835294e-01,\n",
       "           8.04931998e-01,   8.90437901e-01,   1.00932944e+00,\n",
       "           9.54741597e-01,   1.09163404e+00,   1.05546892e+00,\n",
       "           1.01837039e+00,   9.61176753e-01,   9.70594049e-01,\n",
       "           1.05178440e+00,   1.05399489e+00,   1.02932525e+00,\n",
       "           1.06646967e+00,   1.04654586e+00,   7.76960671e-01,\n",
       "           1.04160488e+00,   1.05207658e+00,   9.06163156e-01,\n",
       "           1.03730559e+00,   1.01943934e+00,   9.69452262e-01,\n",
       "           1.05625200e+00,   1.03455734e+00,   9.90581393e-01,\n",
       "           1.05402112e+00,   1.01247525e+00,   8.95767510e-01,\n",
       "           9.66406286e-01,   1.04732156e+00,   1.04005265e+00,\n",
       "           1.04448140e+00,   7.87407279e-01,   9.83068943e-01,\n",
       "           9.83406067e-01,   9.42852914e-01,   1.05188012e+00,\n",
       "           7.68296242e-01,   5.86119294e-01,   1.03821850e+00,\n",
       "           1.00032568e+00,   9.98092711e-01,   1.05825388e+00,\n",
       "           9.20254052e-01,   9.80937600e-01,   9.87281621e-01,\n",
       "           1.01528454e+00,   1.05566716e+00,   1.16750169e+00,\n",
       "           9.76921260e-01,   1.02727163e+00,   9.23954189e-01,\n",
       "           9.39195216e-01,   1.04621458e+00,   1.04435849e+00,\n",
       "           1.06171334e+00,   9.65954900e-01,   1.02075005e+00,\n",
       "           9.33377564e-01,   9.72008824e-01,   1.05587101e+00,\n",
       "           1.05024076e+00,   1.02850473e+00,   1.05712128e+00,\n",
       "           9.31894183e-01,   1.01639259e+00,   1.09356511e+00,\n",
       "           1.01703644e+00,   9.52621400e-01,   1.06327689e+00,\n",
       "           9.86917078e-01,   1.02460325e+00,   8.39294255e-01,\n",
       "           1.01955926e+00,   1.08812451e+00,   1.08003652e+00,\n",
       "           7.97009587e-01,   9.61174250e-01,   1.03967881e+00,\n",
       "           9.65011120e-01,   1.07647562e+00,   1.06267965e+00,\n",
       "           8.41411769e-01,   1.01547706e+00,   1.07737851e+00,\n",
       "           9.91416395e-01,   9.88816261e-01,   1.05750227e+00,\n",
       "           9.97356236e-01,   8.88915479e-01,   1.00406992e+00,\n",
       "           1.04891062e+00,   1.08945060e+00,   1.00430274e+00,\n",
       "           1.03841269e+00,   9.32233751e-01,   1.02372825e+00,\n",
       "           1.05240750e+00,   1.05578291e+00,   1.07622385e+00,\n",
       "           1.04520190e+00,   1.02517354e+00,   1.02209616e+00,\n",
       "           9.64805901e-01,   1.04133034e+00,   1.05802786e+00,\n",
       "           1.09316266e+00,   1.00868356e+00,   8.78953099e-01,\n",
       "           1.12958634e+00,   9.81806517e-01,   1.01699352e+00,\n",
       "           9.62009013e-01,   1.05114830e+00,   7.87638664e-01,\n",
       "           1.05127978e+00,   1.04935336e+00,   1.01254702e+00,\n",
       "           1.09014463e+00,   1.07525718e+00,   1.03677380e+00,\n",
       "           7.39566565e-01,   1.01365304e+00,   1.06440330e+00,\n",
       "           7.86103308e-01,   1.06163919e+00,   1.06327999e+00,\n",
       "           9.82458413e-01,   1.03604078e+00,   8.20519328e-01,\n",
       "           1.10163963e+00,   1.05607498e+00,   1.05294430e+00,\n",
       "           8.67335439e-01,   1.02484465e+00,   1.06553531e+00,\n",
       "           8.77124608e-01,   1.01159394e+00,   9.51592207e-01,\n",
       "           1.05236447e+00,   1.05539703e+00,   1.04782295e+00,\n",
       "           1.04616261e+00,   1.04860282e+00,   8.88748050e-01,\n",
       "           1.05481184e+00,   9.45393682e-01,   8.55442822e-01,\n",
       "           1.05621397e+00,   9.87028837e-01,   1.04954493e+00,\n",
       "           1.07776475e+00,   9.67450082e-01,   1.01306081e+00,\n",
       "           1.00620961e+00,   9.33851302e-01,   1.03016937e+00,\n",
       "           1.05905962e+00,   9.53357935e-01,   8.52428675e-01,\n",
       "           1.06036294e+00,   1.04159629e+00,   1.04710913e+00,\n",
       "           9.68756199e-01,   1.02043164e+00,   9.58153784e-01,\n",
       "           9.70104337e-01,   8.56282473e-01,   1.06610203e+00,\n",
       "           8.18818569e-01,   1.03756452e+00,   1.04479456e+00,\n",
       "           1.00800848e+00,   9.93686438e-01,   8.08499932e-01,\n",
       "           9.98733401e-01,   1.00771058e+00,   1.03033769e+00,\n",
       "           1.01985574e+00,   9.84865010e-01,   1.06870091e+00,\n",
       "           9.39625680e-01,   9.86569047e-01,   1.03084385e+00,\n",
       "           1.01505899e+00,   1.08907366e+00,   1.03109217e+00,\n",
       "           1.02179503e+00,   1.18927300e+00,   8.96217227e-01,\n",
       "           1.07160616e+00,   8.29554260e-01,   1.05384970e+00,\n",
       "           1.07664692e+00,   1.27021745e-01,   1.89764462e-02,\n",
       "          -3.75904441e-02,   4.83702868e-02,  -1.39669552e-01,\n",
       "          -1.14739062e-02,   9.59609076e-02,  -1.27136827e-01,\n",
       "          -1.23476405e-02,   2.33821608e-02,  -1.59922659e-01,\n",
       "          -1.28293764e-02,   1.24087438e-01,  -1.09910227e-01,\n",
       "           3.57323810e-02,  -1.45755913e-02,   1.09607399e-01,\n",
       "          -1.29889539e-02,   4.60643992e-02,   5.81078641e-02,\n",
       "          -3.62645052e-02,   9.53076500e-03,  -6.02201000e-02,\n",
       "          -7.76103092e-03,  -8.55982825e-02,  -2.21120357e-03,\n",
       "          -1.73092298e-02,   8.14341381e-03,  -6.76438659e-02,\n",
       "           5.87992668e-02,  -8.39216337e-02,  -1.75619535e-02,\n",
       "           2.50776336e-02,   9.35151998e-04,  -9.86339897e-02,\n",
       "           1.59366578e-01,   7.90830553e-02,  -2.31903978e-02,\n",
       "           2.79592015e-02,  -1.18719777e-02,   2.20911857e-02,\n",
       "          -3.26511078e-02,  -7.63980381e-04,   2.68601198e-02,\n",
       "          -7.21923560e-02,   4.74096164e-02,   2.14317553e-02,\n",
       "           6.18919469e-02,  -1.53440190e-02,  -6.50932640e-02,\n",
       "          -1.11423889e-02,   2.23948415e-02,   5.40936599e-03,\n",
       "           1.00835212e-01,   6.03256300e-02,  -4.53397669e-02,\n",
       "          -8.83862600e-02,  -3.21489535e-02,  -2.42654346e-02,\n",
       "          -5.04316837e-02,  -2.96964757e-02,   8.32550153e-02,\n",
       "           8.20485801e-02,  -1.04510896e-01,  -4.39999364e-02,\n",
       "           5.30290268e-02,  -1.35766998e-01,  -8.64064321e-02,\n",
       "           9.35580507e-02,  -1.20317005e-03,  -1.40645504e-01,\n",
       "           1.11461841e-02,   2.19968364e-01,  -1.21098151e-02,\n",
       "          -6.38370728e-03,   2.63332743e-02,  -5.04005291e-02,\n",
       "           4.47523333e-02,  -1.40640706e-01,  -2.32615173e-02,\n",
       "          -8.41758586e-03,  -1.81108296e-01,  -1.89255163e-01,\n",
       "           1.01074032e-01,  -1.41815752e-01,  -3.43660973e-02,\n",
       "           4.26268391e-02,  -6.10983968e-02,  -4.17998247e-02,\n",
       "          -2.36424841e-02,   1.15697011e-02,   3.93587276e-02,\n",
       "           3.83145660e-02,  -5.60746118e-02,   3.63642536e-02,\n",
       "          -4.42008749e-02,  -1.95955969e-02,  -3.91895659e-02,\n",
       "          -1.85012430e-01,  -2.70378198e-02,   4.36417945e-02,\n",
       "           5.76541051e-02,   1.89528354e-02,   4.29097749e-02,\n",
       "           1.62332609e-01,   3.05860639e-02,  -3.08430009e-02,\n",
       "           4.89710197e-02,  -5.36271557e-03,   8.43221974e-03,\n",
       "           1.43311918e-02,   5.46331629e-02,  -2.58409865e-02,\n",
       "           3.01101264e-02,  -7.73501843e-02,  -3.83155793e-01,\n",
       "           7.37886876e-03,  -2.73435898e-02,   8.69265348e-02,\n",
       "           5.30724823e-02,  -1.36022288e-02,  -2.10659709e-02,\n",
       "          -4.76781949e-02,   8.62971544e-02,   1.31349325e-01,\n",
       "          -1.98608842e-02,   6.00428134e-02,  -9.81829017e-02,\n",
       "          -1.46281734e-01,  -5.58990128e-02,  -1.54626682e-01,\n",
       "           9.90460142e-02,   2.36736387e-02,  -3.12417764e-02,\n",
       "           5.58238924e-02,  -1.05046574e-02,   6.69426844e-02,\n",
       "          -1.54834632e-02,   7.31916055e-02,   5.26983552e-02,\n",
       "           9.33357403e-02,   5.42224832e-02,  -6.17817417e-02,\n",
       "           7.67421722e-02,  -4.04594131e-02,  -3.63420881e-02,\n",
       "          -5.13105206e-02,   4.52827588e-02,  -6.16445392e-02,\n",
       "           9.81377736e-02,   6.61535636e-02,   1.21270992e-01,\n",
       "           9.24750939e-02,  -1.20802052e-01,   6.63960353e-02,\n",
       "           5.77497259e-02,  -2.73810066e-02,   5.64781995e-03,\n",
       "           4.09641676e-02,   9.86071154e-02,  -1.73299145e-02,\n",
       "          -5.03564067e-03,  -3.12436316e-02,  -1.04278944e-01,\n",
       "           9.45794359e-02,  -1.69944972e-01,  -6.23809099e-02,\n",
       "          -4.79535274e-02,  -6.51668459e-02,  -5.43090794e-03,\n",
       "          -6.79310933e-02,  -3.16326246e-02,  -1.35477278e-02,\n",
       "          -8.30455199e-02,   1.56115629e-02,   1.99833795e-01,\n",
       "           4.75867130e-02,  -6.01372421e-02,  -1.03322886e-01,\n",
       "           2.74828039e-02,   7.13890120e-02,  -2.16643289e-02,\n",
       "           6.65974338e-03,  -2.44758241e-02,   5.41106611e-02,\n",
       "           2.05122344e-02,   6.73068315e-02,  -5.59930317e-02,\n",
       "           4.96857576e-02,  -1.33943081e-01,   2.11746320e-02,\n",
       "          -1.82851106e-01,   1.81410629e-02,   1.69275656e-01,\n",
       "           1.06768847e-01,  -4.54087928e-02,   8.41266960e-02,\n",
       "           6.02512211e-02,   7.44582787e-02,   1.95346668e-01,\n",
       "          -8.45893547e-02,   1.49263479e-02,   1.56764999e-01,\n",
       "           1.41758531e-01,   2.07317039e-01,   1.00943670e-01,\n",
       "           1.57611132e-01,   1.71675086e-01,   6.75640181e-02,\n",
       "          -3.65682691e-02,   2.15137646e-01,  -9.95264947e-02,\n",
       "           4.91161905e-02,   7.26933870e-03,   1.25441682e-02,\n",
       "           1.29331565e-02,  -3.80225778e-02,  -4.88258386e-03,\n",
       "           2.42816526e-02,  -4.70329449e-02,  -3.75786275e-02,\n",
       "           2.35401779e-01,  -2.83134705e-03,   3.68392989e-02,\n",
       "           1.30172461e-01,  -8.83299783e-02,  -1.00515537e-01,\n",
       "           9.14503336e-02,  -8.90345424e-02,  -7.93578401e-02,\n",
       "           1.82709873e-01,   8.05788860e-02,  -2.64399759e-02,\n",
       "           4.96283770e-02,   1.49041206e-01,   2.47342209e-03,\n",
       "          -1.40726585e-02,  -4.40566568e-03,   1.86417192e-01,\n",
       "           8.50253329e-02,   6.45608380e-02,   3.99390087e-02,\n",
       "           1.10125588e-02,   8.36738199e-02,   6.66327029e-02,\n",
       "          -1.07945688e-02,   5.46578737e-03,   5.76343350e-02,\n",
       "          -7.77661577e-02,   8.07392895e-02,   1.21449651e-02,\n",
       "           1.92490555e-02,  -7.99723417e-02,  -2.55207028e-02,\n",
       "           1.92863405e-01,   7.26119727e-02,   3.03759389e-02,\n",
       "           6.33196607e-02,   5.36936633e-02,   4.97088060e-02,\n",
       "          -9.02293250e-02,   6.56083133e-03,  -6.46837987e-03,\n",
       "           3.67671549e-02,   3.49171199e-02,   1.07979082e-01,\n",
       "          -1.37346536e-02,  -6.54931813e-02,   1.14746742e-01,\n",
       "           3.78495082e-02,   3.12046617e-01,   1.29530746e-02,\n",
       "           1.21086612e-02,  -2.16181260e-02,   9.72416773e-02,\n",
       "          -2.38630641e-02,   5.24513312e-02,   3.16132605e-02,\n",
       "           1.15374655e-01,  -7.84946699e-03,  -5.73681295e-02,\n",
       "          -4.11758432e-04,   3.62148076e-01,   8.91256332e-02,\n",
       "          -1.06500937e-02,   8.46547186e-02,  -5.40680997e-02,\n",
       "          -1.45443855e-02,   7.33644143e-02,   1.77590996e-02,\n",
       "          -1.43114273e-02,   2.48702466e-02,   2.05045752e-02,\n",
       "          -2.77256295e-02,  -4.73281480e-02,   1.92743853e-01,\n",
       "          -9.82019119e-03,   2.18683816e-02,   6.34564012e-02,\n",
       "          -1.16793653e-02,   3.57185788e-02,   1.06070533e-01,\n",
       "          -3.13141607e-02,   6.96887076e-03,  -4.94742990e-02,\n",
       "          -6.53477758e-02,   8.43697879e-03,  -6.44882619e-02,\n",
       "           5.46685047e-02,   3.83411511e-03,   5.29114045e-02,\n",
       "           7.68926553e-03,  -5.45501225e-02,   1.30002704e-02,\n",
       "           1.12532966e-01,   2.64072213e-02,  -5.02483861e-04,\n",
       "           1.85507294e-02,   7.30319023e-02,  -7.49432817e-02,\n",
       "           1.36752024e-01,   4.59955493e-03,  -3.11927125e-02,\n",
       "           6.79178312e-02,   1.45347100e-02,  -8.19699168e-02,\n",
       "          -9.72592011e-02,   1.63135722e-01,  -2.10516118e-02,\n",
       "           7.12509826e-03,   1.59358308e-01,  -8.13582242e-02,\n",
       "          -4.78174537e-03,   8.67931694e-02,   2.19160062e-03,\n",
       "           4.83999215e-02,  -2.75980011e-02,  -4.72009256e-02,\n",
       "          -1.15695007e-01,   1.33896038e-01,  -1.24374576e-01,\n",
       "          -1.86373908e-02,   6.52780309e-02,  -3.84770893e-02,\n",
       "           1.35468006e-01,  -6.77846521e-02,  -2.77275164e-02,\n",
       "           7.02030957e-02,   7.48011470e-02,  -4.06155959e-02,\n",
       "           3.03440876e-02,  -2.83169858e-02,  -9.10415687e-03,\n",
       "           2.06888407e-01,  -4.52791452e-02,   7.29512051e-02,\n",
       "          -6.73830211e-02,   2.21897522e-03,   1.22788817e-01,\n",
       "           3.39460783e-02,   7.59603158e-02,   1.33916978e-02,\n",
       "          -9.53551102e-03,  -6.56658644e-03,   2.47476250e-02,\n",
       "           1.04803577e-01,  -9.09419879e-02,   1.68453250e-02,\n",
       "          -6.02202155e-02,   8.87989700e-02,  -4.75306325e-02,\n",
       "           1.27063505e-02,   6.24328665e-02,   1.62927732e-01,\n",
       "           7.27292895e-02,   1.14643604e-01,   7.32806623e-02,\n",
       "          -5.09412847e-02,  -5.76945469e-02,   2.14542039e-02,\n",
       "           2.20263854e-01,   1.70514006e-02,  -7.44315190e-03,\n",
       "          -1.17627807e-01,  -7.38543198e-02,   4.16588970e-02,\n",
       "           9.03410371e-04,   6.47468632e-03,   1.28101818e-02,\n",
       "          -1.28103876e-02,  -5.82311885e-04,  -2.33767331e-02,\n",
       "          -2.44242866e-02,   7.61179999e-02,  -5.71368635e-02,\n",
       "           8.41882378e-02,  -9.85306501e-02,   9.51636508e-02,\n",
       "          -1.12113860e-02,   2.23993529e-02], dtype=float32)],\n",
       " [array([[ 0.21353669, -0.1089435 ,  0.3081038 , ...,  0.14289634,\n",
       "           0.11918837,  0.48870406],\n",
       "         [-0.13796453,  0.12869799, -0.10533366, ..., -0.57167637,\n",
       "           0.34589511, -0.65011185],\n",
       "         [-0.51696074,  0.76148731, -0.26268408, ...,  0.14548732,\n",
       "          -0.55247521, -0.9567098 ],\n",
       "         ..., \n",
       "         [ 1.72669065, -0.22419845, -0.03448119, ..., -0.2350916 ,\n",
       "           0.14488862,  0.32399008],\n",
       "         [-0.18128133, -0.05598602, -0.58447105, ..., -0.31655413,\n",
       "          -0.13478436, -0.51468724],\n",
       "         [ 0.00821043, -0.21708988, -0.43214259, ..., -0.26841637,\n",
       "           0.01823444, -0.22934206]], dtype=float32),\n",
       "  array([-0.29287416,  0.01689857, -0.17109935, -0.10192458, -0.24612179,\n",
       "         -0.21440484, -0.10476691, -0.15642178, -0.21043666, -0.21372302,\n",
       "         -0.2231985 , -0.19042899, -0.18152666,  0.06011411, -0.02436366,\n",
       "          0.03718566, -0.07217249,  0.06498991, -0.01625852, -0.00299914,\n",
       "          0.10594665,  0.104518  , -0.17475946, -0.09411729,  0.04962791,\n",
       "          0.05801455,  0.02450418,  0.06221249, -0.01559973, -0.20704351,\n",
       "         -0.01723072,  0.06921212,  0.08217502, -0.01040281, -0.13188528,\n",
       "          0.07105941, -0.15381931,  0.01148649, -0.19206353], dtype=float32)]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal matrix with no temperature\n",
    "filename = \"weights-improvement-50-1.8065.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "weight_mat = []\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    weight_mat.append(weights)\n",
    "    \n",
    "    \n",
    "weight_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with temp = 1.5\n",
    "filename = \"weights-improvement-235-0.2900.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "temp = 1.5\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    for w in weights:\n",
    "        w *= temp\n",
    "    layer.set_weights(weights)\n",
    "    \n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "poem = \"shall I compare thee to a summer's day?\\n\"\n",
    "\n",
    "poem_ints = []\n",
    "for char in poem:\n",
    "    poem_ints.append(char_map_to_int[char])\n",
    "poem_ints = np.asarray(poem_ints)\n",
    "poem_floats = poem_ints / 39.0\n",
    "\n",
    "f = open(\"poemtemp1.5.txt\", \"w+\")\n",
    "\n",
    "for i in range(440):\n",
    "    g = np.reshape(poem_floats, (1, len(poem_floats), 1))\n",
    "    p = model.predict(g)\n",
    "    new_int = np.argmax(p)\n",
    "    poem_ints = np.append(poem_ints, new_int)\n",
    "    \n",
    "    new_char = int_map_to_char[new_int]\n",
    "    poem += new_char\n",
    "    \n",
    "    new_float = new_int / 39.0\n",
    "    poem_floats = poem_floats[1:40]\n",
    "    poem_floats = np.append(poem_floats, new_float)\n",
    "    print(len(poem_floats))\n",
    "    \n",
    "    \n",
    "f.write(poem)   \n",
    "f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
